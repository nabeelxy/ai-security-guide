# Red Teaming
Red-teaming is a common practice for mitigating unsafe behaviors in Large Language
Models (LLMs), LLM based systems and agents. It involves thoroughly assessing thse systems to identify potential flaws and
addressing them with responsible and accurate responses.


## Recent Publications
| Date |Title | Venue | Material | Tags | Code | Summary |
|---|---|---|---|---|---|---|
| Jun 2025 | AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models | arXiv | [Paper](https://arxiv.org/pdf/2506.14682) | ctf, prompt injection, model evasion, benchmark | [GitHub](https://github.com/dreadnode/AIRTBench-Code) | |
| Apr 2025 | An Approach to Technical AGI Safety and Security | Google | [Report](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf) | security risks, misuse, misalignment | | |
| Mar 2024 | Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem | TDSC | [Paper](https://arxiv.org/pdf/2403.03593) | MaleficNet2 | | |
| Nov 2023 | MART: Improving LLM Safety with Multi-round Automatic Red-Teaming | arXiv, Meta | [Paper](https://arxiv.org/pdf/2311.07689) | Safety Fine-Tuning | | |
| Jun 2022 | EvilModel 2.0: Bringing Neural Network Models into Malware Attacks | arXiv | [Paper](https://arxiv.org/pdf/2109.04344) | malware, stegenography, real-world-attack | | |
| Mar 2022 | MaleficNet: Hiding Malware into Deep Neural Networks using Spread-Spectrum Channel Coding | ESORICS 2022 | [Slides](https://www.sintef.no/contentassets/7db3ab21cd764ba1a336842e39fe236d/03.-michael-alexander-riegler---onde-nevrale-nettverk.pdf) | malware, stegenography | | |
| Aug 2021 | EvilModel: Hiding Malware Inside of Neural Network Models | arXiv | [Paper](https://arxiv.org/pdf/2107.08590) | malware, stegenography | | |
