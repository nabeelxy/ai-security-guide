# Red Teaming
Red-teaming is a common practice for mitigating unsafe behaviors in Large Language
Models (LLMs), LLM based systems and agents. It involves thoroughly assessing thse systems to identify potential flaws and
addressing them with responsible and accurate responses.


## Recent Publications
| Date |Title | Venue | Material | Tags | Code | Summary |
|---|---|---|---|---|---|---|
| Jan 2026 | Building Production-Ready Probes For Gemini | Google DeepMind | [Paper](https://arxiv.org/pdf/2601.11516) [NotebookLM](https://notebooklm.google.com/notebook/01c14191-4cc0-4595-b5d5-b5466f4e8a71)| proboes, misuse-detection, alphaevolve | | [Review](https://github.com/nabeelxy/ai-security-guide/blob/main/security_for_ai/red_teaming/reviews/gemini_probes_jan_2025.md)|
| Jan 2026 | Next-generation Constitutional Classifiers: More efficient protection against universal jailbreaks | Anthropic | [Paper](https://arxiv.org/pdf/2601.04603) [NotebookLM](https://notebooklm.google.com/notebook/ab0eff56-dfd1-4991-9553-07b22a96844b)| probes, jailbreak, defense-in-depth | | [Review](https://github.com/nabeelxy/ai-security-guide/blob/main/security_for_ai/red_teaming/reviews/constitutional_classifiers_plusplus.md)|
| Dec 2025 | COMPARING AI AGENTS TO CYBERSECURITY PROFESSIONALS IN REAL-WORLD PENETRATION TESTING | Stanford | [Paper](https://arxiv.org/pdf/2512.09882v1) [NotebookLM](https://notebooklm.google.com/notebook/9c839956-adae-4c79-b284-f4ccfd7214e1)| agents, pentesting | | [Review](https://github.com/nabeelxy/ai-security-guide/blob/main/security_for_ai/red_teaming/reviews/ai_agents_vs_cs_prof.md)|
| Nov 2025 | EVALUATING ADVERSARIAL VULNERABILITIES IN MODERN LARGE LANGUAGE MODELS | arXiv | [Paper](https://www.arxiv.org/pdf/2511.17666) | self-bypass, cross-bypass | | [Review](https://github.com/nabeelxy/ai-security-guide/blob/main/security_for_ai/red_teaming/reviews/llm_vul_eval_nov_2025.md)|
| Jun 2025 | AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models | arXiv | [Paper](https://arxiv.org/pdf/2506.14682) | ctf, prompt injection, model evasion, benchmark | [GitHub](https://github.com/dreadnode/AIRTBench-Code) | |
| Apr 2025 | An Approach to Technical AGI Safety and Security | Google | [Report](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf) | security risks, misuse, misalignment | | |
| Mar 2024 | Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem | TDSC | [Paper](https://arxiv.org/pdf/2403.03593) | MaleficNet2 | | |
| Nov 2023 | MART: Improving LLM Safety with Multi-round Automatic Red-Teaming | arXiv, Meta | [Paper](https://arxiv.org/pdf/2311.07689) | Safety Fine-Tuning | | |
| Sep 2023 | A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks| arXiv | [Paper](https://arxiv.org/pdf/2308.14367) | LLM, prompt injection, backdoor | | |
| Jun 2022 | EvilModel 2.0: Bringing Neural Network Models into Malware Attacks | arXiv | [Paper](https://arxiv.org/pdf/2109.04344) | malware, steganography, real-world-attack | | |
| Mar 2022 | MaleficNet: Hiding Malware into Deep Neural Networks using Spread-Spectrum Channel Coding | ESORICS 2022 | [Slides](https://www.sintef.no/contentassets/7db3ab21cd764ba1a336842e39fe236d/03.-michael-alexander-riegler---onde-nevrale-nettverk.pdf) [Paper](https://arxiv.org/pdf/2107.08590)| malware, stegenography | | |
| Aug 2021 | EvilModel: Hiding Malware Inside of Neural Network Models | arXiv | [Paper](https://arxiv.org/pdf/2107.08590) | malware, steganography | | |


## Blogs
* 2026-01-09 - [Blog](https://www.anthropic.com/research/next-generation-constitutional-classifiers) Next-generation Constitutional Classifiers: More efficient protection against universal jailbreaks by Anthropic. #ai #safety #defense-in-depth
* 2025-10-28 - Offsec Evals: Growing Up In The Dark Forest by Shane Caldwell. [Link](https://hackbot.dad/writing/offsec-evals-dark-forest/) #eval #offensive-ai-con
* 2025-06-19 - Hunting Vulnerabilities in Keras Model Deserialization [Link](https://blog.huntr.com/hunting-vulnerabilities-in-keras-model-deserialization) #keras #lambda #vulnerability
* 2025-04-04 - Securing Machine Learning Models: A Comprehensive Guide to Model Scanning by Repello [Link](https://repello.ai/blog/securing-machine-learning-models-a-comprehensive-guide-to-model-scanning) #introduction #survey
* 2025-03-27 -  Malicious AI Models Undermine Software Supply-Chain Security [Link](https://cacm.acm.org/research/malicious-ai-models-undermine-software-supply-chain-security/) #ACM-Communications #comprehensive #integration
* 2025-02-06 - Malicious ML models discovered on Hugging Face platform [Link](https://www.reversinglabs.com/blog/rl-identifies-malware-ml-model-hosted-on-hugging-face) #pickle #malware #serialization
* 2024-06-11 - Exploiting ML models with pickle file attacks: Part 1 by Trail of Bits [Link](https://blog.trailofbits.com/2024/06/11/exploiting-ml-models-with-pickle-file-attacks-part-1/) #pickle
* 2024-10-08 - AI Security: Model Serialization Attacks [Link](https://themlsecopshacker.com/p/ai-security-model-serialization-attacks) #serialization
* 2023-08-05 - Enhancing AI/ML Model Security: Effective Strategies to Protect Against Cyber Attacks [Link](https://thanseefpp.medium.com/fortifying-your-ai-ml-models-strategies-to-safeguard-against-cyber-attacks-7183e10d5b75) #ModelScan
* 2022-12-06 - Weaponizing ML Models with Ransomware [Link](https://hiddenlayer.com/innovation-hub/weaponizing-machine-learning-models-with-ransomware/) #steganograpy #pickle
