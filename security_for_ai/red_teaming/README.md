# Red Teaming
Red-teaming is a common practice for mitigating unsafe behaviors in Large Language
Models (LLMs), LLM based systems and agents. It involves thoroughly assessing thse systems to identify potential flaws and
addressing them with responsible and accurate responses.


## Recent Publications
| Date |Title | Venue | Material | Tags | Code | Summary |
|---|---|---|---|---|---|---|
| Jun 2025 | AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models | arXiv | [Paper](https://arxiv.org/pdf/2506.14682) | ctf, prompt injection, model evasion, benchmark | [GitHub](https://github.com/dreadnode/AIRTBench-Code) | |
| Apr 2025 | An Approach to Technical AGI Safety and Security | Google | [Report](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf) | security risks, misuse, misalignment | | |
| Nov 2023 | MART: Improving LLM Safety with Multi-round Automatic Red-Teaming | arXiv, Meta | [Paper](https://arxiv.org/pdf/2311.07689) | Safety Fine-Tuning | | |
| Aug 2021 | EvilModel: Hiding Malware Inside of Neural Network Models | arXiv | [Paper](https://arxiv.org/pdf/2107.08590) | malware, stegenography | | |
