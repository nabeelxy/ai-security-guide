# Red Teaming
Red-teaming is a common practice for mitigating unsafe behaviors in Large Language
Models (LLMs), LLM based systems and agents. It involves thoroughly assessing thse systems to identify potential flaws and
addressing them with responsible and accurate responses.


## Recent Publications
| Date |Title | Venue | Material | Tags | Code | Summary |
|---|---|---|---|---|---|---|
| Jun 2025 | AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models | arXiv | [Paper](https://arxiv.org/pdf/2506.14682) | ctf, prompt injection, model evasion, benchmark | [GitHub](https://github.com/dreadnode/AIRTBench-Code) | |
| Apr 2025 | An Approach to Technical AGI Safety and Security | Google | [Report](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf) | security risks, misuse, misalignment | | |
| Mar 2024 | Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem | TDSC | [Paper](https://arxiv.org/pdf/2403.03593) | MaleficNet2 | | |
| Nov 2023 | MART: Improving LLM Safety with Multi-round Automatic Red-Teaming | arXiv, Meta | [Paper](https://arxiv.org/pdf/2311.07689) | Safety Fine-Tuning | | |
| Sep 2023 | A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks| arXiv | [Paper](https://arxiv.org/pdf/2308.14367) | LLM, prompt injection, backdoor | | |
| Jun 2022 | EvilModel 2.0: Bringing Neural Network Models into Malware Attacks | arXiv | [Paper](https://arxiv.org/pdf/2109.04344) | malware, steganography, real-world-attack | | |
| Mar 2022 | MaleficNet: Hiding Malware into Deep Neural Networks using Spread-Spectrum Channel Coding | ESORICS 2022 | [Slides](https://www.sintef.no/contentassets/7db3ab21cd764ba1a336842e39fe236d/03.-michael-alexander-riegler---onde-nevrale-nettverk.pdf) [Paper](https://arxiv.org/pdf/2107.08590)| malware, stegenography | | |
| Aug 2021 | EvilModel: Hiding Malware Inside of Neural Network Models | arXiv | [Paper](https://arxiv.org/pdf/2107.08590) | malware, steganography | | |


## Blogs
* 2025-06-19 - Hunting Vulnerabilities in Keras Model Deserialization [Link](https://blog.huntr.com/hunting-vulnerabilities-in-keras-model-deserialization) #keras #lambda #vulnerability
* 2025-04-04 - Securing Machine Learning Models: A Comprehensive Guide to Model Scanning by Repello [Link](https://repello.ai/blog/securing-machine-learning-models-a-comprehensive-guide-to-model-scanning) #introduction #survey
* 2025-03-27 - https://cacm.acm.org/research/malicious-ai-models-undermine-software-supply-chain-security/ [Link](https://cacm.acm.org/research/malicious-ai-models-undermine-software-supply-chain-security/) #ACM-Communications #comprehensive #integration
* 2025-02-06 - Malicious ML models discovered on Hugging Face platform [Link](https://www.reversinglabs.com/blog/rl-identifies-malware-ml-model-hosted-on-hugging-face) #pickle #malware #serialization
* 2024-06-11 - Exploiting ML models with pickle file attacks: Part 1 by Trail of Bits [Link](https://blog.trailofbits.com/2024/06/11/exploiting-ml-models-with-pickle-file-attacks-part-1/) #pickle
* 2024-10-08 - AI Security: Model Serialization Attacks [Link](https://themlsecopshacker.com/p/ai-security-model-serialization-attacks) #serialization
* 2022-12-06 - Weaponizing ML Models with Ransomware [Link](https://hiddenlayer.com/innovation-hub/weaponizing-machine-learning-models-with-ransomware/) #steganograpy #pickle
