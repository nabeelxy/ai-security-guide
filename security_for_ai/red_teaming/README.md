# Red Teaming
Red-teaming is a common practice for mitigating unsafe behaviors in Large Language
Models (LLMs), LLM based systems and agents. It involves thoroughly assessing thse systems to identify potential flaws and
addressing them with responsible and accurate responses.


## Recent Publications
| Date | ID | Title | Venue | Material | Tags | Short Summary | Summary |
|---|---|---|---|---|---|---|---|
| Apr 2025 | G_Saftey_Security | An Approach to Technical AGI Safety and Security | Google | [Report](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf) | security risks, misuse, misalignment | | |
| Nov 2023 | MART | MART: Improving LLM Safety with Multi-round Automatic Red-Teaming | arXiv, Meta | [Paper](https://arxiv.org/pdf/2311.07689) | Safety Fine-Tuning | | |
