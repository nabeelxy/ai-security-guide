# Safety Bias and Fairness
This section explores the safety, bias and fairness concerns of AI models.

## Recent Publications
| Date | Title | Venue | Material | Tags | Code | Summary |
| --- | --- | --- | --- | --- | --- | --- |
| Apr 2025 | Real-World Gaps in AI Governance Research | arXiv | [Paper](https://arxiv.org/pdf/2505.00174) | pre-deployment, post-deployment, alignment, bias | | |
| Dec 2024 | ALIGNMENT FAKING IN LARGE LANGUAGE MODELS | Anthropic | [Paper](https://arxiv.org/pdf/2412.14093) [NotebookLM](https://notebooklm.google.com/notebook/209fda8f-2ead-4277-8788-e4e4e15dbba2)| alignment, faking | | |
| Nov 2024 | Evaluating sparse autoencoders on targeted concept erasure tasks | NeurIPS | [Paper](https://arxiv.org/pdf/2411.18895) | | | |
| Oct 2024 | RLEF: GROUNDING CODE LLMS IN EXECUTION FEEDBACK WITH REINFORCEMENT LEARNING | arXiv, Meta | [Paper](https://arxiv.org/pdf/2410.02089v1) | | | |
| Oct 2024 | Image-Perfect Imperfections: Safety, Bias and Authenticity in the Shadow of Text-To-Image Model Evolution | arXiv| [Paper](https://arxiv.org/pdf/2408.17285)| | |

## Blogs
* [12-18-2024] [Blog](https://www.anthropic.com/research/alignment-faking) Alignment Faking in LLMs by Anthropic. #alignment #faking #security #safety
