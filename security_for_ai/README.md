# Security for AI
In this section, we explore various security threats and defenses to AI models including traditional ML, DL and GenAI (LLMs).

Specifically, we keep a close eye on the following areas:
* [LLM Security](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/llm_security)
* [Adversarial Machine Learning](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/adversarial_machine_learning)
* [Explainability](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/explainability)
* [Safety, Bias and Fairness](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/safety_bias_fairness)
* [RAG Security](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/rag_security)
* [Agents Security](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/agents_security)
* [Model Supply Chain Security](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/model_supply_chain_security)
* [Benchmarks](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/benchmarks)
* [Red Teaming](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/red_teaming)

We also track the [security startups](https://github.com/nabeelxy/ai-security-guide/tree/main/security_for_ai/startups) that defend AI.
