# Hallucination

Hallucination occurs when a large language model generates output that is factually incorrect, nonsensical, or unfaithful to source material, yet presents it with high confidence and plausible structure. This phenomenon stems from the model’s fundamental nature as a probabilistic token predictor; rather than accessing a database of verified facts, the LLM predicts the next statistically likely word based on training patterns, leading it to "fill in the gaps" with fabricated details—such as fake citations, events, or code libraries—when it lacks specific knowledge.

