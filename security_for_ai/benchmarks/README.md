# Benchmarks
In this section, we keep track of security benchmark datasets, especially those created to evaluate generative models. These benchmark datasets fall under the following cateories:
* Fairness and Bias
* Privacy
* Adversarial robustness
* Out of Distribution (OOD) robustness
* Safety alignment
* Over refusal
* Hallucination
* Code security

## Recent Publications

| Date | Title | Venue | Material | Tags | Short Summary | Summary |
|---|---|---|---|---|---|---|
| Aug 2025 | HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns | Usenix Security 2025 | [Paper](https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-893-shen.pdf) | hate speech, adversarial attack | | |
| Aug 2024 | Cyberseceval 3: Advancing the evaluation of cybersecurity risks and capabilities in large language models | arXiv, Meta | [Paper](https://arxiv.org/pdf/2408.01605) | offensive security, social engineering, code security | | |
| Sep 2018 | Hotpotqa: A dataset for diverse, explainable multi-hop question answering | arXiv | [Paper](https://arxiv.org/pdf/1809.09600) |  Wikipedia based QA, hallucination | | |
