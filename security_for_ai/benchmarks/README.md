# Benchmarks
In this section, we keep track of security benchmark datasets, especially those created to evaluate generative models. These benchmark datasets fall under the following cateories:
* Fairness and Bias
* Privacy
* Adversarial robustness
* Out of Distribution (OOD) robustness
* Safety alignment
* Over refusal
* Hallucination
* Code security

## Recent Publications

| Date | ID | Title | Venue | Material | Tags | Short Summary | Summary |
|---|---|---|---|---|---|---|---|
| Aug 2024 | CyberSecEval 3 | Cyberseceval 3: Advancing the evaluation of cybersecurity risks and capabilities in large language models | arXiv, Meta | [Paper](https://arxiv.org/pdf/2408.01605) | offensive security, social engineering, code security | | |
| Sep 2018 | HotPotQA | Hotpotqa: A dataset for diverse, explainable multi-hop question answering | arXiv | [Paper](https://arxiv.org/pdf/1809.09600) |  Wikipedia based QA, hallucination | | |
